{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a0458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d24cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8ce1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from utils import rotate_preserve_size\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from layers import mlp, Patches, PatchEncoder\n",
    "from loss import angular_loss_mae\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from transformers import ViTFeatureExtractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8c3059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "IMAGE_SIZE = 224\n",
    "patch_size = 16  # Size of the patches to be extract from the input images\n",
    "num_patches = (IMAGE_SIZE // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]\n",
    "input_shape = (224, 224, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "558e9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotGenerator(Sequence):\n",
    "    def __init__(self, image_dir, batch_size, dim, channels_first=False, is_vit=False):\n",
    "        self.files = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.channels_first = channels_first\n",
    "        self.is_vit = is_vit\n",
    "        \n",
    "    def __len__(self):\n",
    "        if len(self.files) % self.batch_size == 0:\n",
    "            return len(self.files) // self.batch_size\n",
    "        return len(self.files) // self.batch_size + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_slice = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
    "        batch_files = self.files[batch_slice]\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for i, f in enumerate(batch_files):\n",
    "            try:\n",
    "                angle = float(np.random.choice(range(0, 360)))\n",
    "                img = rotate_preserve_size(f, angle, (self.dim, self.dim))\n",
    "                img = np.array(img)\n",
    "                if self.is_vit:\n",
    "                    X.append(img)\n",
    "                else:\n",
    "                    if self.channels_first:\n",
    "                        img = img.transpose(2, 0, 1)\n",
    "\n",
    "                    img = np.expand_dims(img, axis=0)\n",
    "                    X.append(img)\n",
    "                    # X[i] = img\n",
    "                    # y[i] = angle\n",
    "                y.append(angle)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if self.is_vit:\n",
    "            X = feature_extractor(images=X, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "            X = np.array(X)\n",
    "            if not self.channels_first:\n",
    "                X = X.transpose(0, 2, 3, 1)\n",
    "        else:\n",
    "            X = np.concatenate(X, axis=0)\n",
    "        y = np.array(y)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.files)\n",
    "\n",
    "\n",
    "# In[83]:\n",
    "\n",
    "\n",
    "class ValidationTestGenerator(Sequence):\n",
    "    def __init__(self, image_dir, df_label_path, batch_size, dim, mode, channels_first=False, is_vit=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.mode = mode\n",
    "        self.channels_first = channels_first\n",
    "        self.is_vit = is_vit\n",
    "        \n",
    "        df_label = pd.read_csv(df_label_path)\n",
    "        self.df = df_label[df_label[\"mode\"] == self.mode].reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        total = self.df.shape[0]\n",
    "        if total % self.batch_size == 0:\n",
    "            return total // self.batch_size\n",
    "        return total // self.batch_size + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_slice = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
    "        df_batch = self.df[batch_slice].reset_index(drop=True).copy()\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for i in range(len(df_batch)):\n",
    "            try:\n",
    "                angle = df_batch.angle[i]\n",
    "                path = os.path.join(self.image_dir, df_batch.image[i])\n",
    "                img = rotate_preserve_size(path, angle, (self.dim, self.dim))\n",
    "\n",
    "                img = np.array(img)\n",
    "                if self.is_vit:\n",
    "                    X.append(img)\n",
    "                else:\n",
    "                    if self.channels_first:\n",
    "                        img = img.transpose(2, 0, 1)\n",
    "\n",
    "                    img = np.expand_dims(img, axis=0)\n",
    "                    X.append(img)\n",
    "                y.append(angle)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if self.is_vit:\n",
    "            X = feature_extractor(images=X, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "            X = np.array(X)\n",
    "            if not self.channels_first:\n",
    "                X = X.transpose(0, 2, 3, 1)\n",
    "        else:\n",
    "            X = np.concatenate(X, axis=0)\n",
    "        y = np.array(y)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "96d62b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanPOS(layers.Layer):\n",
    "    def __init__(self, image_size):\n",
    "        super(EuclideanPOS, self).__init__()\n",
    "        self.w = self.add_weight(shape=(image_size, image_size, 3), trainable=True, initializer=\"glorot_normal\")\n",
    "        \n",
    "        cx, cy = ((image_size-1)/2, (image_size-1)/2)\n",
    "        xcoords, ycoords = tf.meshgrid(np.arange(image_size).astype(\"float32\"), \n",
    "                                       np.arange(image_size).astype(\"float32\"))\n",
    "        self.euc_pos = tf.sqrt((xcoords - cx)**2 + (ycoords - cy)**2) / image_size\n",
    "        self.euc_pos = tf.expand_dims(self.euc_pos, axis=-1)\n",
    "        \n",
    "    def call(self, img_arr):\n",
    "        return img_arr + self.w + self.euc_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce5a8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_model():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    \n",
    "    euc_pos_encoded = EuclideanPOS(IMAGE_SIZE)(inputs)\n",
    "    \n",
    "    patches = Patches(patch_size)(euc_pos_encoded)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    y = layers.Dense(1, activation=\"linear\")(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=y)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ffd1f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "euclidean_pos_4 (EuclideanPOS)  (None, 224, 224, 3)  150528      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patches_5 (Patches)             (None, None, 768)    0           euclidean_pos_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_5 (PatchEncoder)  (None, 196, 64)      61760       patches_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, 196, 64)      128         patch_encoder_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_40 (MultiH (None, 196, 64)      66368       layer_normalization_85[0][0]     \n",
      "                                                                 layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 196, 64)      0           multi_head_attention_40[0][0]    \n",
      "                                                                 patch_encoder_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_86 (LayerNo (None, 196, 64)      128         add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 196, 128)     8320        layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 196, 128)     0           dense_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_102 (Dense)               (None, 196, 64)      8256        dropout_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 196, 64)      0           dense_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 196, 64)      0           dropout_96[0][0]                 \n",
      "                                                                 add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_87 (LayerNo (None, 196, 64)      128         add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_41 (MultiH (None, 196, 64)      66368       layer_normalization_87[0][0]     \n",
      "                                                                 layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 196, 64)      0           multi_head_attention_41[0][0]    \n",
      "                                                                 add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_88 (LayerNo (None, 196, 64)      128         add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 196, 128)     8320        layer_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 196, 128)     0           dense_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_104 (Dense)               (None, 196, 64)      8256        dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 196, 64)      0           dense_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 196, 64)      0           dropout_98[0][0]                 \n",
      "                                                                 add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_89 (LayerNo (None, 196, 64)      128         add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_42 (MultiH (None, 196, 64)      66368       layer_normalization_89[0][0]     \n",
      "                                                                 layer_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_84 (Add)                    (None, 196, 64)      0           multi_head_attention_42[0][0]    \n",
      "                                                                 add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_90 (LayerNo (None, 196, 64)      128         add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_105 (Dense)               (None, 196, 128)     8320        layer_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 196, 128)     0           dense_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_106 (Dense)               (None, 196, 64)      8256        dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, 196, 64)      0           dense_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_85 (Add)                    (None, 196, 64)      0           dropout_100[0][0]                \n",
      "                                                                 add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_91 (LayerNo (None, 196, 64)      128         add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_43 (MultiH (None, 196, 64)      66368       layer_normalization_91[0][0]     \n",
      "                                                                 layer_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_86 (Add)                    (None, 196, 64)      0           multi_head_attention_43[0][0]    \n",
      "                                                                 add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_92 (LayerNo (None, 196, 64)      128         add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_107 (Dense)               (None, 196, 128)     8320        layer_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 196, 128)     0           dense_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_108 (Dense)               (None, 196, 64)      8256        dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, 196, 64)      0           dense_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_87 (Add)                    (None, 196, 64)      0           dropout_102[0][0]                \n",
      "                                                                 add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_93 (LayerNo (None, 196, 64)      128         add_87[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_44 (MultiH (None, 196, 64)      66368       layer_normalization_93[0][0]     \n",
      "                                                                 layer_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_88 (Add)                    (None, 196, 64)      0           multi_head_attention_44[0][0]    \n",
      "                                                                 add_87[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_94 (LayerNo (None, 196, 64)      128         add_88[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_109 (Dense)               (None, 196, 128)     8320        layer_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 196, 128)     0           dense_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_110 (Dense)               (None, 196, 64)      8256        dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 196, 64)      0           dense_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_89 (Add)                    (None, 196, 64)      0           dropout_104[0][0]                \n",
      "                                                                 add_88[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_95 (LayerNo (None, 196, 64)      128         add_89[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_45 (MultiH (None, 196, 64)      66368       layer_normalization_95[0][0]     \n",
      "                                                                 layer_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_90 (Add)                    (None, 196, 64)      0           multi_head_attention_45[0][0]    \n",
      "                                                                 add_89[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_96 (LayerNo (None, 196, 64)      128         add_90[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_111 (Dense)               (None, 196, 128)     8320        layer_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 196, 128)     0           dense_111[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_112 (Dense)               (None, 196, 64)      8256        dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)           (None, 196, 64)      0           dense_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_91 (Add)                    (None, 196, 64)      0           dropout_106[0][0]                \n",
      "                                                                 add_90[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_97 (LayerNo (None, 196, 64)      128         add_91[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_46 (MultiH (None, 196, 64)      66368       layer_normalization_97[0][0]     \n",
      "                                                                 layer_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_92 (Add)                    (None, 196, 64)      0           multi_head_attention_46[0][0]    \n",
      "                                                                 add_91[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_98 (LayerNo (None, 196, 64)      128         add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_113 (Dense)               (None, 196, 128)     8320        layer_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 196, 128)     0           dense_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_114 (Dense)               (None, 196, 64)      8256        dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)           (None, 196, 64)      0           dense_114[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_93 (Add)                    (None, 196, 64)      0           dropout_108[0][0]                \n",
      "                                                                 add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_99 (LayerNo (None, 196, 64)      128         add_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_47 (MultiH (None, 196, 64)      66368       layer_normalization_99[0][0]     \n",
      "                                                                 layer_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_94 (Add)                    (None, 196, 64)      0           multi_head_attention_47[0][0]    \n",
      "                                                                 add_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_100 (LayerN (None, 196, 64)      128         add_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_115 (Dense)               (None, 196, 128)     8320        layer_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 196, 128)     0           dense_115[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_116 (Dense)               (None, 196, 64)      8256        dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 196, 64)      0           dense_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_95 (Add)                    (None, 196, 64)      0           dropout_110[0][0]                \n",
      "                                                                 add_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_101 (LayerN (None, 196, 64)      128         add_95[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12544)        0           layer_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 12544)        0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_117 (Dense)               (None, 2048)         25692160    dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)           (None, 2048)         0           dense_117[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_118 (Dense)               (None, 1024)         2098176     dropout_112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)           (None, 1024)         0           dense_118[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_119 (Dense)               (None, 1)            1025        dropout_113[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 28,669,377\n",
      "Trainable params: 28,669,377\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_vit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7f5c03ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "   3/7393 [..............................] - ETA: 4:30:56 - loss: 91.4143"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     10\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    852\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    853\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    858\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[1;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m     args,\n\u001b[1;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1923\u001b[0m     executing_eagerly)\n\u001b[1;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss=angular_loss_mae, optimizer=Adadelta(learning_rate=0.1))\n",
    "\n",
    "train_gen = RotGenerator(\"/data/chandanp/train2017/\", 16, IMAGE_SIZE, is_vit=True)\n",
    "val_gen = ValidationTestGenerator(image_dir=\"/data/subhadip/validation-test/\", \n",
    "                                  df_label_path=\"/data/subhadip/validation-test.csv\",\n",
    "                                  batch_size=32, dim=IMAGE_SIZE, mode=\"valid\", is_vit=True)\n",
    "cp = ModelCheckpoint(\"/data/subhadip/weights/model-euc-vit-ang-loss.h5\", save_weights_only=False, \n",
    "                     save_best_only=True, monitor=\"loss\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5)\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=10000, callbacks=[cp, es, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a287c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
