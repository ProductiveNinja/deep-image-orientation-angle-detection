{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from utils import rotate_preserve_size\n",
    "from loss import angular_loss_mae\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers as L\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications import Xception, EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from loguru import logger\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.optimizers import Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16711680/16705208 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Define conv base\n",
    "# conv_base = Xception(weights=\"imagenet\", include_top=False, input_shape=(299, 299, 3))\n",
    "conv_base = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(299, 299, 3))\n",
    "for layer in conv_base.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
      "_________________________________________________________________\n",
      "efficientnetb0 (Functional)  (None, 10, 10, 1280)      4049571   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               65536512  \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 69,737,252\n",
      "Trainable params: 69,693,565\n",
      "Non-trainable params: 43,687\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "img_input = L.Input(shape=(299, 299, 3))\n",
    "x = conv_base(img_input)\n",
    "x = L.Flatten()(x)\n",
    "x = L.Dense(512, activation=\"relu\")(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dense(256, activation=\"relu\")(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dense(64, activation=\"relu\")(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "y = L.Dense(1, activation=\"linear\")(x)\n",
    "model = Model(img_input, y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Generator\n",
    "class RotGenerator(Sequence):\n",
    "    def __init__(self, image_dir, batch_size, dim):\n",
    "        self.files = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        \n",
    "    def __len__(self):\n",
    "        if len(self.files) % self.batch_size == 0:\n",
    "            return len(self.files) // self.batch_size\n",
    "        return len(self.files) // self.batch_size + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_slice = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
    "        batch_files = self.files[batch_slice]\n",
    "        \n",
    "        X = np.zeros(shape=(len(batch_files), self.dim, self.dim, 3))\n",
    "        y = np.zeros(shape=(len(batch_files), ))\n",
    "        \n",
    "        for i, f in enumerate(batch_files):\n",
    "            # img = cv2.imread(f)\n",
    "            angle = float(np.random.choice(range(0, 360)))\n",
    "            img = rotate_preserve_size(f, angle, (self.dim, self.dim))\n",
    "            \n",
    "            X[i] = img\n",
    "            y[i] = angle\n",
    "        \n",
    "        return X/255., y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.files)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Generator\n",
    "class ValidationTestGenerator(Sequence):\n",
    "    def __init__(self, image_dir, df_label_path, batch_size, dim, mode):\n",
    "        self.image_dir = image_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.mode = mode\n",
    "        \n",
    "        df_label = pd.read_csv(df_label_path)\n",
    "        self.df = df_label[df_label[\"mode\"] == self.mode].reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        total = self.df.shape[0]\n",
    "        if total % self.batch_size == 0:\n",
    "            return total // self.batch_size\n",
    "        return total // self.batch_size + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_slice = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
    "        df_batch = self.df[batch_slice].reset_index(drop=True).copy()\n",
    "        \n",
    "        X = np.zeros(shape=(len(df_batch), self.dim, self.dim, 3))\n",
    "        y = np.zeros(shape=(len(df_batch), ))\n",
    "        \n",
    "        for i in range(len(df_batch)):\n",
    "            angle = df_batch.angle[i]\n",
    "            path = os.path.join(self.image_dir, df_batch.image[i])\n",
    "            img = rotate_preserve_size(path, angle, (self.dim, self.dim))\n",
    "            \n",
    "            X[i] = img\n",
    "            y[i] = angle\n",
    "        \n",
    "        return X/255., y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f679fdf8940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - ETA: 0s - loss: 79.5585"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 01:21:14.243655: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 137319936 exceeds 10% of free system memory.\n",
      "2022-03-28 01:21:16.820848: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 137319936 exceeds 10% of free system memory.\n",
      "2022-03-28 01:21:17.309088: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.73GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3/3 [==============================] - 37s 15s/step - loss: 79.8537 - val_loss: 91.1038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f676c401a60>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model.compile(loss=angular_loss_mae, optimizer=Adadelta(learning_rate=0.1))\n",
    "\n",
    "train_gen = RotGenerator(\"/data/chandanp/train2017/\", 32, 299)\n",
    "val_gen = ValidationTestGenerator(image_dir=\"/data/subhadip/data/validation-test/\", \n",
    "                                  df_label_path=\"/data/subhadip/data/validation-test.csv\",\n",
    "                                  batch_size=128, dim=299, mode=\"valid\")\n",
    "cp = ModelCheckpoint(\"/data/subhadip/weights/model-en-ang-loss.h5\", save_weights_only=False, \n",
    "                     save_best_only=True, monitor=\"loss\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5)\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=10000, callbacks=[cp, es, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
