{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 18 00:25:03 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.94       Driver Version: 470.94       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 35%   61C    P2    65W / 250W |   7709MiB / 11177MiB |     10%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 70%   84C    P2   198W / 250W |  10281MiB / 11178MiB |     92%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 75%   85C    P2   230W / 250W |  10931MiB / 11178MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 39%   57C    P8    15W / 250W |      4MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1412      G   /usr/lib/xorg/Xorg                 16MiB |\n",
      "|    0   N/A  N/A     21355      C   ...da3/envs/myenv/bin/python     6935MiB |\n",
      "|    0   N/A  N/A     23648      C   ...iran/anaconda3/bin/python      753MiB |\n",
      "|    1   N/A  N/A     16810      C   python                          10277MiB |\n",
      "|    2   N/A  N/A      1085      C   python                          10927MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 00:25:14.696835: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from transformers import TFAutoModel\n",
    "from utils import rotate_preserve_size\n",
    "from loss import angular_loss_mae\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers as L\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications import Xception, EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from loguru import logger\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from generator import RotGenerator, ValidationTestGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "class ViTRotGenerator(Sequence):\n",
    "    def __init__(self, image_dir, batch_size, dim):\n",
    "        self.files = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        \n",
    "    def __len__(self):\n",
    "        if len(self.files) % self.batch_size == 0:\n",
    "            return len(self.files) // self.batch_size\n",
    "        return len(self.files) // self.batch_size + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_slice = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
    "        batch_files = self.files[batch_slice]\n",
    "\n",
    "        X_conv = []\n",
    "        X_vit = []\n",
    "        y = []\n",
    "        \n",
    "        for i, f in enumerate(batch_files):\n",
    "            try:\n",
    "                angle = float(np.random.choice(range(0, 360)))\n",
    "                img = rotate_preserve_size(f, angle, (self.dim, self.dim))\n",
    "                img = np.array(img)\n",
    "                X_vit.append(img)\n",
    "\n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                X_conv.append(img)\n",
    "                y.append(angle)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        X_vit = feature_extractor(images=X_vit, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        X_vit = np.array(X_vit)\n",
    "        X_conv = np.concatenate(X_conv, axis=0)\n",
    "        y = np.array(y)\n",
    "\n",
    "        return [X_vit, X_conv], y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTValidationTestGenerator(Sequence):\n",
    "    def __init__(self, image_dir, df_label_path, batch_size, dim, mode, channels_first=False, is_vit=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.mode = mode\n",
    "        self.channels_first = channels_first\n",
    "        self.is_vit = is_vit\n",
    "        \n",
    "        df_label = pd.read_csv(df_label_path)\n",
    "        self.df = df_label[df_label[\"mode\"] == self.mode].reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        total = self.df.shape[0]\n",
    "        if total % self.batch_size == 0:\n",
    "            return total // self.batch_size\n",
    "        return total // self.batch_size + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_slice = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
    "        df_batch = self.df[batch_slice].reset_index(drop=True).copy()\n",
    "        \n",
    "\n",
    "        X_conv = []\n",
    "        X_vit = []\n",
    "        y = []\n",
    "        \n",
    "        for i in range(len(df_batch)):\n",
    "            try:\n",
    "                angle = df_batch.angle[i]\n",
    "                path = os.path.join(self.image_dir, df_batch.image[i])\n",
    "                img = rotate_preserve_size(path, angle, (self.dim, self.dim))\n",
    "\n",
    "                img = np.array(img)\n",
    "                X_vit.append(img)\n",
    "\n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                X_conv.append(img)\n",
    "                y.append(angle)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        X_vit = feature_extractor(images=X_vit, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        X_vit = np.array(X_vit)\n",
    "        X_conv = np.concatenate(X_conv, axis=0)\n",
    "        y = np.array(y)\n",
    "\n",
    "        return [X_vit, X_conv], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 00:25:26.092286: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-18 00:25:26.093245: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-18 00:25:26.129915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-04-18 00:25:26.129954: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-18 00:25:26.214365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-18 00:25:26.214504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-04-18 00:25:26.250437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-18 00:25:26.377127: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-18 00:25:26.414691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-18 00:25:26.464493: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-18 00:25:26.512492: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-18 00:25:26.514907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-18 00:25:26.515539: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-18 00:25:26.518019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-04-18 00:25:26.518085: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-18 00:25:26.518143: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-18 00:25:26.518170: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-04-18 00:25:26.518195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-18 00:25:26.518221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-18 00:25:26.518245: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-18 00:25:26.518271: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-18 00:25:26.518296: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-18 00:25:26.522059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-18 00:25:26.522136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-18 00:25:27.130657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-18 00:25:27.130692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-04-18 00:25:27.130699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-04-18 00:25:27.135871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10269 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1)\n",
      "2022-04-18 00:25:27.136184: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-18 00:25:27.575905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-18 00:25:28.485609: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "Some layers from the model checkpoint at google/vit-base-patch16-224 were not used when initializing TFViTModel: ['classifier']\n",
      "- This IS expected if you are initializing TFViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit/pooler/dense/kernel:0', 'vit/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# get ViT base model\n",
    "vit_base = TFAutoModel.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=224\n",
    "PATCH_SIZE = 16\n",
    "PROJECTION_DIM = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CONV base model\n",
    "conv_base = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "for layer in conv_base.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchAttention(L.Layer):\n",
    "    def __init__(self, projection_dim):\n",
    "        super(PatchAttention, self).__init__()\n",
    "        self.mha = L.MultiHeadAttention(num_heads=1, key_dim=projection_dim, dropout=0.1)\n",
    "        \n",
    "    def call(self, encoded_patches, image_size, patch_size):\n",
    "        batch_size = tf.shape(encoded_patches)[0]\n",
    "        max_seq_len = tf.shape(encoded_patches)[1]\n",
    "        # x = L.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        x = encoded_patches\n",
    "        \n",
    "        _, attention_weights = self.mha(x, x, return_attention_scores=True)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, max_seq_len, max_seq_len))\n",
    "        attention_weights = tf.math.reduce_mean(attention_weights, axis=1)\n",
    "        \n",
    "        # Removing CLS token\n",
    "        attention_weights = attention_weights[:,1:]\n",
    "        patches = image_size // patch_size\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, patches, patches))\n",
    "\n",
    "        # move to image space\n",
    "        pixel_weights = tf.repeat(attention_weights, repeats=[patch_size], axis=-1)\n",
    "        pixel_weights = tf.repeat(pixel_weights, repeats=[patch_size], axis=1)\n",
    "        pixel_weights = tf.expand_dims(pixel_weights, axis=-1)\n",
    "        \n",
    "        return pixel_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchAttentionV2(L.Layer):\n",
    "    def __init__(self, projection_dim):\n",
    "        super(PatchAttentionV2, self).__init__()\n",
    "        self.mha = L.MultiHeadAttention(num_heads=1, key_dim=projection_dim)\n",
    "        \n",
    "    def call(self, encoded_patches, image_size, patch_size):\n",
    "        batch_size = tf.shape(encoded_patches)[0]\n",
    "        \n",
    "        _, attention_weights = self.mha(encoded_patches, encoded_patches, return_attention_scores=True)\n",
    "        attention_weights = tf.squeeze(attention_weights, axis=1)\n",
    "        attention_weights = attention_weights[:, 1:, 0]\n",
    "        attention_weights = tf.nn.sigmoid(attention_weights)\n",
    "        \n",
    "        patches = image_size // patch_size\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, patches, patches))\n",
    "\n",
    "        # move to image space\n",
    "        pixel_weights = tf.repeat(attention_weights, repeats=[patch_size], axis=-1)\n",
    "        pixel_weights = tf.repeat(pixel_weights, repeats=[patch_size], axis=1)\n",
    "        pixel_weights = tf.expand_dims(pixel_weights, axis=-1)\n",
    "        \n",
    "        return pixel_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method PatchAttention.call of <__main__.PatchAttention object at 0x7f36dc515dc0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method PatchAttention.call of <__main__.PatchAttention object at 0x7f36dc515dc0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 3, 224, 224) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_vi_t_model (TFViTModel)      TFBaseModelOutputWit 86389248    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patch_attention_1 (PatchAttenti (None, None, None, 1 2362368     tf_vi_t_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 224, 224, 3)  0           patch_attention_1[0][0]          \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "efficientnetb0 (Functional)     (None, 7, 7, 1280)   4049571     multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 62720)        0           efficientnetb0[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          32113152    flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          131328      batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256)          1024        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           16448       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64)           256         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 64)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            65          flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 125,065,508\n",
      "Trainable params: 121,014,273\n",
      "Non-trainable params: 4,051,235\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "vit_input = L.Input(shape=(3,IMAGE_SIZE, IMAGE_SIZE))\n",
    "conv_input = L.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "vit_out = vit_base(vit_input)\n",
    "pixel_weights = PatchAttention(PROJECTION_DIM)(vit_out[0], IMAGE_SIZE, PATCH_SIZE)\n",
    "\n",
    "x = L.Multiply()([pixel_weights, conv_input])\n",
    "x = conv_base(x)\n",
    "x = L.Flatten()(x)\n",
    "x = L.Dense(512, activation=\"relu\")(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dense(256, activation=\"relu\")(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dense(64, activation=\"relu\")(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Flatten()(x)\n",
    "y = L.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "model = Model([vit_input, conv_input], y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_vi_t_model/vit/pooler/dense/kernel:0', 'tf_vi_t_model/vit/pooler/dense/bias:0', 'patch_attention_1/multi_head_attention_1/value/kernel:0', 'patch_attention_1/multi_head_attention_1/value/bias:0', 'patch_attention_1/multi_head_attention_1/attention_output/kernel:0', 'patch_attention_1/multi_head_attention_1/attention_output/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_vi_t_model/vit/pooler/dense/kernel:0', 'tf_vi_t_model/vit/pooler/dense/bias:0', 'patch_attention_1/multi_head_attention_1/value/kernel:0', 'patch_attention_1/multi_head_attention_1/value/bias:0', 'patch_attention_1/multi_head_attention_1/attention_output/kernel:0', 'patch_attention_1/multi_head_attention_1/attention_output/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x7f36c2cc89d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 22s 454ms/step - loss: 93.3403\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
      "Epoch 2/10000\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 90.8688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     10\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1145\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1142\u001b[0m   val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1143\u001b[0m   epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1145\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py:428\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    427\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(callback, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_supports_tf_logs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 428\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m numpy_logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Only convert once.\u001b[39;00m\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py:1344\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1344\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py:1393\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest \u001b[38;5;241m=\u001b[39m current\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_weights_only:\n\u001b[0;32m-> 1393\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1396\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(filepath, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2108\u001b[0m, in \u001b[0;36mModel.save_weights\u001b[0;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[1;32m   2106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   2107\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m-> 2108\u001b[0m     hdf5_format\u001b[38;5;241m.\u001b[39msave_weights_to_hdf5_group(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2110\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/h5py/_hl/files.py:461\u001b[0m, in \u001b[0;36mFile.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;129m@with_phil\u001b[39m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid:\n\u001b[0;32m--> 461\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/subhadip/anaconda3/envs/myenv/lib/python3.8/site-packages/h5py/_hl/files.py:443\u001b[0m, in \u001b[0;36mFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id_ \u001b[38;5;129;01min\u001b[39;00m file_list:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m id_\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[0;32m--> 443\u001b[0m         \u001b[43mh5i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_ref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    446\u001b[0m _objects\u001b[38;5;241m.\u001b[39mnonlocal_close()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss=angular_loss_mae, optimizer=Adadelta(learning_rate=0.1))\n",
    "\n",
    "train_gen = ViTRotGenerator(\"/data/chandanp/train2017/\", 16, IMAGE_SIZE)\n",
    "val_gen = ViTValidationTestGenerator(image_dir=\"/data/subhadip/data/\", \n",
    "                                     df_label_path=\"/data/subhadip/data/validation-test.csv\",\n",
    "                                     batch_size=16, dim=IMAGE_SIZE, mode=\"valid\")\n",
    "cp = ModelCheckpoint(\"/data/subhadip/weights/model-vit-en-ang-loss.h5\", save_weights_only=True, \n",
    "                     save_best_only=True, monitor=\"loss\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5)\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=10000, callbacks=[cp, es, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 268ms/step - loss: 82.1580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82.15802764892578"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_gen, steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
